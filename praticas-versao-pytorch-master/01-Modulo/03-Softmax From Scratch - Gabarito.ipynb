{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizado Profundo - UFMG\n",
    "\n",
    "## Preâmbulo\n",
    "\n",
    "O código abaixo consiste dos imports comuns. Além do mais, configuramos as imagens para ficar de um tamanho aceitável e criamos algumas funções auxiliares. No geral, você pode ignorar a próxima célula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf8\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams['figure.figsize']  = (18, 10)\n",
    "plt.rcParams['axes.labelsize']  = 20\n",
    "plt.rcParams['axes.titlesize']  = 20\n",
    "plt.rcParams['legend.fontsize'] = 20\n",
    "plt.rcParams['xtick.labelsize'] = 20\n",
    "plt.rcParams['ytick.labelsize'] = 20\n",
    "plt.rcParams['lines.linewidth'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()\n",
    "\n",
    "plt.style.use('seaborn-colorblind')\n",
    "plt.rcParams['figure.figsize']  = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código de GD abaixo está pré-preparado para a primeira parte da tarefa. Não precisa mudar o mesmo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(d_fun, loss_fun, X, y, lambda_=0.01, tol=0.00001, max_iter=10000):\n",
    "    theta = torch.ones(Y.shape[1],X.shape[1])\n",
    "    theta.requires_grad_(True)\n",
    "    #print('Iter {}; theta = '.format(0), theta)\n",
    "    \n",
    "    old_lf = np.inf\n",
    "    i = 0\n",
    "    while True:\n",
    "        # Computar as derivadas\n",
    "        theta.requires_grad_(True)    \n",
    "        grad = d_fun(X,theta,y)\n",
    "\n",
    "        # Atualizar\n",
    "        with torch.no_grad():\n",
    "            theta_novo = theta - lambda_ * grad\n",
    "        \n",
    "        #Parar quando o erro convergir\n",
    "        lf = loss_fun(X, theta, y)\n",
    "        if torch.abs(old_lf - lf) <= tol:\n",
    "            break\n",
    "        \n",
    "         #Atualizar parâmetros e erro\n",
    "        theta = theta_novo\n",
    "        old_lf = lf\n",
    "        \n",
    "        # Informação de debug\n",
    "        #print('Iter {}; theta = '.format(i+1), theta)\n",
    "        i += 1\n",
    "        if i == max_iter:\n",
    "            break\n",
    "        \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para testar o resultado dos seus algoritmos vamos usar o módulo testing do numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.testing import assert_equal\n",
    "from numpy.testing import assert_almost_equal\n",
    "from numpy.testing import assert_array_almost_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aula 03 - Softmax para Imagens\n",
    "\n",
    "Continuando da aula anterior, vamos implementar uma regressão logística (softmax) para várias classes. Além do mais, vamos fazer uso da mesma para algumas tarefas classificação de imagens.\n",
    "\n",
    "Para iniciar a transição para o mundo de Deep Learning, vamos implementar uma nova logística (não será mais do zero) usando as camadas que o pytorch já traz prontas.\n",
    "\n",
    "!!? (Qual sera o equivalente em pytorch) Antes de iniciar o notebook, sugiro uma revisão do [Capítulo 3](d2l.ai)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax em PyTorch/PyTorch NN\n",
    "\n",
    "A função softmax pode ser utilizada para problemas multiclasse. No exemplo abaixo, temos a função softmax exemplificada. Diferente dos casos anteriores, aqui nós temos uma matriz de parâmetros: $\\mathbf{\\Theta}$. Cada coluna da matriz $\\mathbf{\\Theta}_y$ contém os parâmetros para uma classe $y$.\n",
    "\n",
    "$$p(y|\\mathbf{x}_i, \\mathbf{\\theta}_y) = \\mathrm{softmax}(\\mathbf{x}^t_i) = \\frac{\\exp(\\mathbf{x}^t_i \\mathbf{\\theta}_y)}{\\sum_{y'} \\exp(\\mathbf{x}^t_{i} \\mathbf{\\theta}_{y'})}$$\n",
    "\n",
    "Vamos pensar no caso que temos 10 classes e 20 atributos:\n",
    "  * O tamanho e $\\mathbf{X}$ é `(n, 20)`, `n` é o número de exemplos\n",
    "  * O tamanho e $\\mathbf{\\Theta}$ é `(c, 20)`, onde `c` é o número de classes\n",
    "  * $\\mathbf{\\Theta}^t$ é `(20, c)`\n",
    "  * $\\mathbf{X} \\mathbf{\\Theta}^t$ é `(n, c)`. Uma probabilidade para cada classe. Tal produto interno é representado por uma linha de $\\mathbf{X}$, $\\mathbf{x}^t_i$ multiplicado pela coluna de $\\mathbf{\\Theta}$, $\\mathbf{\\theta}_y$.\n",
    "  \n",
    "O softmax é basicamente uma matriz onde aplicamos uma exponencial em toda célula: $e^{\\mathbf{X} \\mathbf{\\Theta}^t}$. Depois normalizamos por classe. Assim voltamos para probabilidades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro, sem usar o modulo nn do pytorch ainda, implemente:\n",
    "   1. Uma função softmax\n",
    "   1. Uma função de perda\n",
    "   1. Derivadas em pytorch (autograd)\n",
    "\n",
    "Antes disso, vamos usar nossos blobs de sempre. Essa deve ser a última aula com os mesmo :-("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No nosso novo exemplo temos 200 amostras, 20 features e 10 classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "state = np.random.seed(20190187)\n",
    "\n",
    "X, y = datasets.make_blobs(n_samples=200, n_features=20, centers=2)\n",
    "X = torch.tensor(X)\n",
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 20])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(y.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemente a função softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, Theta):\n",
    "    '''\n",
    "    Aqui Theta é a matriz de parâmetros e X uma matriz.\n",
    "    Seu código deve retornar um vetor de previsões para toda linha de X.\n",
    "    '''\n",
    "    P = torch.exp(torch.matmul(X.double(), (Theta.T).double()))\n",
    "    return (P.T / P.sum(axis=1)).T.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testes, não apague!\n",
    "\n",
    "# Se X tem tamanho (n, f) a matriz theta é (c, f). c é o numéro de clases.\n",
    "# X.T é (f, c). Assim X @ Theta.T -> (n, c)\n",
    "# O resultado do softmax é a probabilidade de cada classe\n",
    "\n",
    "Theta = torch.randn(4, X.shape[1])\n",
    "P = softmax(X, Theta)\n",
    "assert_equal((200, 4), P.shape)\n",
    "assert_almost_equal(np.ones(len(P)), P.sum(axis=1).numpy(), 4) # verifica se toda linha soma == 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4867e-08, 1.5216e-01, 9.4605e-11, 8.4784e-01],\n",
       "        [9.9834e-01, 1.6629e-03, 2.5770e-18, 4.8671e-12],\n",
       "        [1.5081e-14, 4.9174e-04, 1.3891e-14, 9.9951e-01],\n",
       "        [3.2386e-04, 9.9968e-01, 1.2864e-15, 9.7512e-14],\n",
       "        [1.0000e+00, 4.7578e-06, 3.5355e-15, 6.3521e-15],\n",
       "        [6.4200e-12, 7.1688e-05, 2.4624e-13, 9.9993e-01],\n",
       "        [9.9810e-01, 1.8981e-03, 1.4810e-11, 3.4749e-10],\n",
       "        [9.9983e-01, 1.7398e-04, 7.9975e-13, 5.0942e-16],\n",
       "        [1.3654e-10, 3.3068e-04, 5.0270e-07, 9.9967e-01],\n",
       "        [3.6507e-01, 6.3493e-01, 4.3364e-14, 3.2655e-11],\n",
       "        [2.8886e-09, 2.9250e-01, 1.2185e-08, 7.0750e-01],\n",
       "        [1.7854e-03, 9.9821e-01, 1.1592e-17, 1.5827e-15],\n",
       "        [9.9998e-01, 2.4518e-05, 6.9354e-13, 1.1573e-10],\n",
       "        [1.9003e-15, 2.5687e-05, 1.8866e-12, 9.9997e-01],\n",
       "        [1.4068e-01, 8.5932e-01, 2.4998e-12, 2.6254e-11],\n",
       "        [3.3574e-05, 9.9997e-01, 2.9717e-19, 3.1242e-11],\n",
       "        [6.3119e-13, 2.8449e-05, 1.8970e-12, 9.9997e-01],\n",
       "        [9.6968e-01, 3.0325e-02, 2.3545e-11, 1.8437e-13],\n",
       "        [1.9332e-13, 4.0138e-08, 1.2683e-14, 1.0000e+00],\n",
       "        [3.7132e-09, 1.6250e-05, 2.8463e-04, 9.9970e-01],\n",
       "        [2.6771e-15, 5.4537e-05, 1.9100e-16, 9.9995e-01],\n",
       "        [2.4307e-13, 2.8410e-04, 4.6230e-08, 9.9972e-01],\n",
       "        [7.2037e-16, 7.6669e-04, 9.3559e-14, 9.9923e-01],\n",
       "        [4.2408e-11, 3.3455e-06, 6.4679e-10, 1.0000e+00],\n",
       "        [9.8244e-01, 1.7558e-02, 3.9503e-13, 4.7210e-10],\n",
       "        [3.7675e-01, 6.2325e-01, 5.5998e-16, 5.3363e-11],\n",
       "        [1.3721e-14, 8.0784e-09, 5.2002e-12, 1.0000e+00],\n",
       "        [3.9938e-13, 4.7396e-06, 7.0436e-13, 1.0000e+00],\n",
       "        [8.7964e-01, 1.2036e-01, 6.7994e-14, 3.5200e-13],\n",
       "        [2.6606e-17, 7.9071e-07, 1.1657e-18, 1.0000e+00],\n",
       "        [9.9358e-01, 6.4250e-03, 2.1094e-15, 8.6059e-14],\n",
       "        [9.9952e-01, 4.8435e-04, 7.9519e-16, 4.1966e-15],\n",
       "        [5.4167e-01, 4.5833e-01, 5.3253e-15, 1.4520e-11],\n",
       "        [8.6385e-01, 1.3615e-01, 3.3629e-17, 5.9964e-14],\n",
       "        [9.2729e-01, 7.2706e-02, 9.7297e-13, 4.6827e-12],\n",
       "        [9.9802e-01, 1.9751e-03, 7.7176e-10, 2.0413e-12],\n",
       "        [8.0961e-01, 1.9039e-01, 7.8857e-13, 1.0646e-14],\n",
       "        [1.6568e-13, 4.7763e-04, 8.6586e-11, 9.9952e-01],\n",
       "        [9.8781e-01, 1.2185e-02, 3.5656e-11, 4.2670e-08],\n",
       "        [9.9999e-01, 1.0446e-05, 7.9282e-16, 6.1931e-14],\n",
       "        [7.6177e-02, 9.2382e-01, 5.2647e-16, 5.9270e-13],\n",
       "        [1.1414e-12, 7.9482e-08, 4.6866e-11, 1.0000e+00],\n",
       "        [4.6627e-10, 6.5516e-05, 7.4479e-07, 9.9993e-01],\n",
       "        [1.2043e-14, 1.5718e-07, 3.7554e-12, 1.0000e+00],\n",
       "        [1.6867e-15, 2.2351e-08, 1.7994e-20, 1.0000e+00],\n",
       "        [7.1925e-11, 8.9788e-05, 9.8262e-13, 9.9991e-01],\n",
       "        [1.4939e-11, 1.1070e-05, 1.6019e-09, 9.9999e-01],\n",
       "        [9.9985e-01, 1.4594e-04, 6.4535e-12, 1.4819e-14],\n",
       "        [7.1451e-17, 2.2699e-10, 1.3083e-18, 1.0000e+00],\n",
       "        [9.7385e-01, 2.6150e-02, 4.3004e-13, 1.0856e-13],\n",
       "        [3.7347e-15, 1.7085e-09, 1.9333e-16, 1.0000e+00],\n",
       "        [9.9099e-01, 9.0104e-03, 3.2453e-13, 7.5125e-12],\n",
       "        [1.7311e-15, 1.0037e-09, 2.5331e-10, 1.0000e+00],\n",
       "        [9.8187e-01, 1.8127e-02, 2.0279e-18, 1.0951e-14],\n",
       "        [7.8444e-14, 5.0946e-08, 1.6792e-08, 1.0000e+00],\n",
       "        [1.8017e-15, 6.6599e-08, 2.5667e-11, 1.0000e+00],\n",
       "        [1.2535e-15, 1.6422e-06, 1.9432e-11, 1.0000e+00],\n",
       "        [9.9998e-01, 1.8717e-05, 3.7055e-13, 8.8942e-14],\n",
       "        [4.6782e-11, 9.7863e-06, 5.7000e-07, 9.9999e-01],\n",
       "        [4.9104e-12, 2.4982e-05, 7.2978e-13, 9.9998e-01],\n",
       "        [1.7106e-12, 5.9692e-06, 4.7771e-09, 9.9999e-01],\n",
       "        [1.8749e-11, 2.1534e-04, 1.3179e-08, 9.9978e-01],\n",
       "        [3.1517e-12, 1.8620e-06, 5.3222e-12, 1.0000e+00],\n",
       "        [1.9391e-02, 9.8061e-01, 3.8554e-18, 3.2119e-13],\n",
       "        [9.0161e-02, 9.0984e-01, 1.1187e-14, 6.7091e-11],\n",
       "        [6.8805e-17, 3.9385e-09, 1.5123e-14, 1.0000e+00],\n",
       "        [3.3940e-11, 1.0165e-04, 6.0411e-08, 9.9990e-01],\n",
       "        [9.9086e-01, 9.1414e-03, 2.6547e-13, 4.7410e-14],\n",
       "        [2.1196e-03, 9.9786e-01, 4.9696e-14, 1.8816e-05],\n",
       "        [3.2946e-10, 1.0190e-04, 1.2484e-09, 9.9990e-01],\n",
       "        [2.0271e-13, 3.6366e-07, 3.9300e-09, 1.0000e+00],\n",
       "        [3.6986e-10, 1.5134e-02, 2.3574e-08, 9.8487e-01],\n",
       "        [1.5839e-03, 9.9842e-01, 2.9425e-17, 2.3442e-08],\n",
       "        [1.7945e-01, 8.2055e-01, 1.4493e-13, 2.4420e-11],\n",
       "        [9.6000e-01, 3.9996e-02, 1.7935e-11, 5.6105e-12],\n",
       "        [6.2180e-13, 1.6644e-05, 3.5640e-11, 9.9998e-01],\n",
       "        [9.9798e-01, 2.0182e-03, 1.9004e-16, 4.2292e-14],\n",
       "        [7.7575e-01, 2.2425e-01, 1.4207e-16, 7.8922e-14],\n",
       "        [1.9618e-01, 8.0382e-01, 2.2901e-14, 9.3491e-10],\n",
       "        [1.0659e-12, 7.5759e-04, 3.3470e-11, 9.9924e-01],\n",
       "        [1.2117e-01, 8.7883e-01, 1.2362e-10, 1.0791e-11],\n",
       "        [4.6602e-01, 5.3398e-01, 2.7216e-14, 7.5447e-11],\n",
       "        [9.7058e-11, 2.9071e-05, 5.1918e-11, 9.9997e-01],\n",
       "        [9.9347e-01, 6.5349e-03, 1.3273e-12, 1.9063e-11],\n",
       "        [9.9351e-01, 6.4891e-03, 1.5726e-11, 2.1977e-11],\n",
       "        [8.8942e-01, 1.1058e-01, 5.3880e-09, 3.6899e-11],\n",
       "        [2.2145e-01, 7.7855e-01, 9.8925e-14, 2.4469e-10],\n",
       "        [8.8417e-01, 1.1583e-01, 7.1475e-15, 1.3943e-13],\n",
       "        [3.7134e-10, 2.1174e-06, 4.6180e-10, 1.0000e+00],\n",
       "        [1.8045e-15, 1.1536e-08, 1.5623e-11, 1.0000e+00],\n",
       "        [5.7361e-01, 4.2639e-01, 1.7201e-15, 3.4327e-09],\n",
       "        [6.1477e-17, 5.1549e-04, 2.5473e-16, 9.9948e-01],\n",
       "        [1.5724e-15, 8.3022e-07, 5.1349e-12, 1.0000e+00],\n",
       "        [1.0594e-11, 1.1103e-05, 1.2514e-09, 9.9999e-01],\n",
       "        [9.9895e-01, 1.0450e-03, 1.4711e-14, 1.3214e-10],\n",
       "        [3.6450e-14, 1.3799e-07, 7.6492e-16, 1.0000e+00],\n",
       "        [5.2067e-01, 4.7933e-01, 6.3618e-12, 1.5519e-10],\n",
       "        [9.9313e-01, 6.8713e-03, 8.5403e-16, 1.8826e-10],\n",
       "        [9.9910e-01, 8.9994e-04, 8.2633e-14, 2.9677e-13],\n",
       "        [8.8139e-01, 1.1861e-01, 3.6405e-14, 1.7121e-14],\n",
       "        [1.3679e-12, 1.5686e-07, 5.7735e-10, 1.0000e+00],\n",
       "        [4.3120e-01, 5.6880e-01, 3.5777e-14, 1.6498e-13],\n",
       "        [3.7353e-14, 3.1415e-06, 1.4895e-14, 1.0000e+00],\n",
       "        [9.9690e-01, 3.1010e-03, 4.3985e-12, 2.7429e-13],\n",
       "        [8.9409e-01, 1.0591e-01, 9.4535e-13, 2.4577e-12],\n",
       "        [6.8945e-12, 1.7263e-05, 3.3628e-10, 9.9998e-01],\n",
       "        [8.3927e-01, 1.6073e-01, 7.7164e-12, 6.3313e-10],\n",
       "        [3.2054e-08, 9.5552e-05, 8.9961e-03, 9.9091e-01],\n",
       "        [2.2448e-15, 1.0080e-08, 2.0752e-13, 1.0000e+00],\n",
       "        [8.1800e-16, 1.3193e-06, 7.0404e-14, 1.0000e+00],\n",
       "        [9.9998e-01, 1.7475e-05, 3.9101e-14, 6.1533e-13],\n",
       "        [1.1350e-16, 7.0764e-05, 3.4158e-14, 9.9993e-01],\n",
       "        [9.9366e-01, 6.3448e-03, 2.4877e-17, 6.3980e-14],\n",
       "        [1.4117e-01, 8.5883e-01, 3.5868e-14, 9.9409e-08],\n",
       "        [1.8871e-04, 9.9981e-01, 1.5168e-16, 2.3830e-14],\n",
       "        [6.8806e-13, 1.3498e-05, 1.9037e-08, 9.9999e-01],\n",
       "        [4.5892e-11, 1.7030e-05, 9.9084e-09, 9.9998e-01],\n",
       "        [2.9305e-15, 5.8137e-10, 1.0782e-11, 1.0000e+00],\n",
       "        [1.4144e-13, 2.0525e-09, 6.0902e-11, 1.0000e+00],\n",
       "        [1.0000e+00, 5.0843e-10, 1.1207e-19, 1.1162e-18],\n",
       "        [7.4170e-16, 5.2486e-08, 1.8894e-10, 1.0000e+00],\n",
       "        [2.1749e-12, 9.4388e-06, 8.1836e-14, 9.9999e-01],\n",
       "        [5.6231e-11, 1.0956e-07, 9.6714e-09, 1.0000e+00],\n",
       "        [4.6840e-13, 5.4094e-03, 6.1746e-10, 9.9459e-01],\n",
       "        [2.6460e-01, 7.3540e-01, 1.5337e-17, 1.1188e-13],\n",
       "        [6.6855e-12, 6.3989e-03, 4.3512e-10, 9.9360e-01],\n",
       "        [9.3189e-01, 6.8114e-02, 6.3968e-16, 2.4688e-13],\n",
       "        [1.7222e-08, 1.5364e-06, 8.1850e-11, 1.0000e+00],\n",
       "        [5.3006e-12, 7.6408e-04, 5.5996e-09, 9.9924e-01],\n",
       "        [9.8475e-01, 1.5248e-02, 1.2684e-16, 1.1479e-14],\n",
       "        [1.2679e-12, 1.3353e-07, 2.8003e-11, 1.0000e+00],\n",
       "        [1.1527e-11, 9.8462e-05, 1.4040e-05, 9.9989e-01],\n",
       "        [3.5133e-01, 6.4867e-01, 2.0263e-11, 3.6165e-09],\n",
       "        [1.7381e-15, 4.8705e-07, 1.7475e-12, 1.0000e+00],\n",
       "        [9.4136e-01, 5.8636e-02, 5.5783e-12, 4.7633e-09],\n",
       "        [9.8502e-01, 1.4984e-02, 7.4039e-18, 4.0811e-13],\n",
       "        [4.4850e-17, 3.9783e-09, 3.6039e-15, 1.0000e+00],\n",
       "        [1.7667e-15, 4.8629e-06, 1.5188e-11, 1.0000e+00],\n",
       "        [9.9949e-01, 5.0578e-04, 7.1570e-13, 3.8676e-14],\n",
       "        [7.3503e-14, 1.8177e-08, 1.2321e-09, 1.0000e+00],\n",
       "        [4.1198e-01, 5.8802e-01, 1.1808e-08, 4.0154e-11],\n",
       "        [9.9724e-01, 2.7622e-03, 1.0988e-12, 1.2215e-11],\n",
       "        [1.1103e-15, 2.1155e-08, 6.4845e-10, 1.0000e+00],\n",
       "        [4.0178e-01, 5.9822e-01, 1.9591e-13, 1.8197e-13],\n",
       "        [1.0000e+00, 3.9793e-06, 6.6697e-11, 9.4171e-13],\n",
       "        [1.6444e-17, 1.0472e-08, 3.0988e-15, 1.0000e+00],\n",
       "        [9.9905e-01, 9.4743e-04, 9.0770e-13, 5.5501e-12],\n",
       "        [9.9961e-01, 3.8769e-04, 3.3800e-15, 2.5212e-16],\n",
       "        [1.0000e+00, 2.5072e-06, 4.5617e-14, 2.3523e-14],\n",
       "        [4.9490e-01, 5.0510e-01, 8.2762e-12, 3.3955e-09],\n",
       "        [8.6926e-14, 4.3344e-05, 5.3567e-15, 9.9996e-01],\n",
       "        [4.6748e-01, 5.3252e-01, 1.6237e-12, 3.2381e-13],\n",
       "        [2.0671e-10, 4.8990e-04, 5.5550e-09, 9.9951e-01],\n",
       "        [9.6541e-01, 3.4589e-02, 5.2011e-14, 3.4386e-14],\n",
       "        [1.5164e-12, 3.4001e-05, 5.0076e-12, 9.9997e-01],\n",
       "        [1.9993e-08, 4.8167e-04, 1.1988e-07, 9.9952e-01],\n",
       "        [9.9584e-01, 4.1604e-03, 3.2050e-13, 1.5899e-11],\n",
       "        [9.4025e-01, 5.9745e-02, 1.3948e-13, 9.9599e-11],\n",
       "        [5.2157e-10, 1.5895e-06, 2.0019e-07, 1.0000e+00],\n",
       "        [5.7135e-13, 1.5260e-06, 3.8527e-12, 1.0000e+00],\n",
       "        [6.8639e-13, 8.8159e-05, 2.9952e-13, 9.9991e-01],\n",
       "        [8.9700e-01, 1.0300e-01, 7.2973e-16, 8.9403e-11],\n",
       "        [9.9492e-01, 5.0782e-03, 4.3900e-13, 1.4264e-13],\n",
       "        [8.1404e-15, 2.1791e-07, 1.4452e-13, 1.0000e+00],\n",
       "        [9.9863e-01, 1.3716e-03, 3.5477e-15, 9.7734e-09],\n",
       "        [1.0000e+00, 1.0319e-07, 2.9490e-16, 1.4833e-15],\n",
       "        [3.9450e-11, 7.5911e-03, 4.5415e-07, 9.9241e-01],\n",
       "        [9.9900e-01, 1.0043e-03, 1.9580e-13, 1.6987e-13],\n",
       "        [4.7367e-16, 1.5169e-05, 1.1340e-16, 9.9998e-01],\n",
       "        [4.7922e-06, 1.0000e+00, 2.0121e-19, 4.0575e-16],\n",
       "        [5.7148e-16, 2.0521e-07, 2.5494e-15, 1.0000e+00],\n",
       "        [1.9165e-12, 1.8268e-07, 4.7362e-10, 1.0000e+00],\n",
       "        [1.3823e-13, 2.5138e-08, 2.2750e-11, 1.0000e+00],\n",
       "        [4.9026e-04, 9.9951e-01, 1.3328e-17, 1.6469e-13],\n",
       "        [8.1634e-13, 4.0063e-06, 9.5565e-11, 1.0000e+00],\n",
       "        [1.3152e-15, 5.0073e-09, 2.6682e-09, 1.0000e+00],\n",
       "        [9.9406e-01, 5.9416e-03, 5.2548e-14, 8.8460e-11],\n",
       "        [2.7009e-14, 3.2158e-05, 4.9993e-12, 9.9997e-01],\n",
       "        [4.8770e-13, 2.5788e-05, 1.6146e-10, 9.9997e-01],\n",
       "        [5.8977e-03, 9.9410e-01, 2.9005e-18, 1.7342e-14],\n",
       "        [2.1724e-01, 7.8276e-01, 2.9270e-16, 3.0428e-12],\n",
       "        [5.4303e-03, 9.9457e-01, 6.7746e-19, 4.7055e-10],\n",
       "        [1.3619e-12, 4.1704e-06, 1.0844e-11, 1.0000e+00],\n",
       "        [3.8722e-14, 1.0969e-04, 2.7809e-10, 9.9989e-01],\n",
       "        [1.1581e-19, 4.7820e-08, 8.9309e-17, 1.0000e+00],\n",
       "        [1.1531e-11, 1.5495e-06, 3.0350e-10, 1.0000e+00],\n",
       "        [1.4468e-04, 9.9986e-01, 6.2905e-19, 2.0215e-14],\n",
       "        [9.6642e-01, 3.3578e-02, 2.3733e-16, 8.2534e-11],\n",
       "        [4.4469e-10, 1.0526e-04, 4.0553e-10, 9.9989e-01],\n",
       "        [8.6720e-01, 1.3280e-01, 8.7261e-10, 7.2626e-12],\n",
       "        [9.3759e-04, 9.9906e-01, 1.2327e-16, 4.6615e-11],\n",
       "        [9.9998e-01, 2.0393e-05, 1.3507e-15, 2.9878e-16],\n",
       "        [8.8946e-01, 1.1054e-01, 8.5882e-15, 1.7892e-10],\n",
       "        [6.1409e-01, 3.8591e-01, 6.0466e-12, 1.6752e-14],\n",
       "        [1.1911e-14, 1.2857e-06, 3.9524e-10, 1.0000e+00],\n",
       "        [5.4940e-19, 5.1554e-11, 3.5260e-14, 1.0000e+00],\n",
       "        [3.3581e-14, 4.1685e-07, 1.5173e-11, 1.0000e+00],\n",
       "        [5.4691e-01, 4.5309e-01, 2.1646e-16, 4.3675e-14],\n",
       "        [3.6235e-12, 1.5183e-08, 1.1180e-12, 1.0000e+00],\n",
       "        [2.8730e-02, 9.7127e-01, 5.4797e-17, 2.4691e-12]], dtype=torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta = torch.randn(4, X.shape[1])\n",
    "P = softmax(X, Theta)\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora implemente uma função de perda de entropia cruzada. Um truque para implementar a mesma é pensar nas respostas como uma matrix $\\mathbf{Y}$. O tamanho de $\\mathbf{Y}$ é `(n, c)`, ou seja, exemplos por classes. Cada vetor das linhas da matriz, simplesmente $\\mathbf{y}_i$, é da forma one-hot `0, 0, 0, 1, 0`. Neste formato, apenas a classe do exemplo está setada como 1, todo o resto é zero. No exemplo, o elemento $i$ é da classe `3` (assumindo que a primeira classe é `0`). Vamos converter nossa resposta y em tal matriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaocouto/anaconda3/envs/temp/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.preprocessing\n",
    "hot = sklearn.preprocessing.OneHotEncoder(sparse=False)\n",
    "# y[:, None] adiciona uma dimensão, vira uma matriz (n, 1). Cada linha é uma classe, e.g.: [[1], [0], [0], ...]\n",
    "# O fit_transform vai converter em uma matriz (n, c).\n",
    "Y = hot.fit_transform(y[:, None]) \n",
    "# fazendo Y ser mxnet\n",
    "Y = torch.tensor(Y)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 2])\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaocouto/anaconda3/envs/temp/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# fazendo X ser um tensor pytorch\n",
    "X = torch.tensor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 20])\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos definir a entropia cruzada do caso multiclasse. Sendo:\n",
    "\n",
    "$$\\hat{\\mathbf{p}}_i = < p(y_0|\\mathbf{x}_i, \\mathbf{\\theta}_y)), \\quad  p(y_1|\\mathbf{x}_i, \\mathbf{\\theta}_y)), \\quad \\cdots, \\quad p(y_{n-1}|\\mathbf{x}_i, \\mathbf{\\theta}_y))>^t$$\n",
    "\n",
    "Um vetor coluna com a probabilidade de cada classe para o exemplo $i$. Ou seja, o vetor tem `c` linhas. $\\mathbf{y}^t_i$ é um vetor linha one-hot: e.g., `0, 0, 0, 1, 0`. O produto interno dos dois retorna a probabilidade da classe correta para cada exemplo: $\\mathbf{y}^t_i \\hat{\\mathbf{p}}_i$. A média do log de cada probabilidade é a entropia cruzada!\n",
    "\n",
    "$$CE(\\theta \\mid y_i, \\mathbf{x}_i) = n^{-1} \\sum_i \\mathbf{y}^t_i \\log(\\, \\hat{\\mathbf{p}}_i\\, )$$\n",
    "\n",
    "Usando sua matriz `Y` acima. Basta fazer `Y * P` (multiplicação por elemento) para zerar qualquer probabilidade da classe errada. Depois disso, um `.sum(axis=1)` faz o produto interno para todos os exemplos.\n",
    "\n",
    "Implemente a função `cross_entropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, Theta, Y):\n",
    "    P = softmax(X, Theta)\n",
    "    p = (Y * P).sum(axis=1)\n",
    "    return -torch.log(p).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2756, -0.2814,  0.7578, -2.6468, -0.8414, -1.5610, -0.9591, -1.0010,\n",
       "         -0.4572, -1.1511,  0.5597, -1.1143,  0.3191, -0.6513, -0.2658,  0.3625,\n",
       "         -1.5865, -1.6491,  0.6605,  0.6632],\n",
       "        [-0.6167, -1.5550,  0.4883, -1.1062, -0.2460,  0.1895, -0.0418,  0.5089,\n",
       "         -1.0788,  0.0986,  0.1188, -0.9229,  1.6993, -0.0484,  0.2143, -0.4027,\n",
       "         -1.1465, -0.5094,  0.9355,  0.9562]], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testes\n",
    "# O valor tem que ser um único número positivo\n",
    "for _ in range(100):\n",
    "    Theta = torch.randn(2, X.shape[1])\n",
    "    assert(loss(X, Theta, Y) >= 0)\n",
    "Theta.requires_grad_(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivadas(X, Theta, Y):\n",
    "    l = loss(X, Theta, Y)\n",
    "    l.backward()\n",
    "    return Theta.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0738, dtype=torch.float64, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(X, Theta, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1366,  0.0673,  0.0330, -0.2354,  0.1519,  0.1490, -0.0028, -0.0539,\n",
       "         -0.0157, -0.2422,  0.0360, -0.2363,  0.0283, -0.1986,  0.1860, -0.1943,\n",
       "         -0.0950,  0.2081, -0.0941,  0.1698],\n",
       "        [ 0.1366, -0.0673, -0.0330,  0.2354, -0.1519, -0.1490,  0.0028,  0.0539,\n",
       "          0.0157,  0.2422, -0.0360,  0.2363, -0.0283,  0.1986, -0.1860,  0.1943,\n",
       "          0.0950, -0.2081,  0.0941, -0.1698]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivadas(X, Theta, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use essa função antes de executar o GD\n",
    "def add_intercept(X):\n",
    "    Xn = torch.zeros(X.shape[0], X.shape[1] + 1)\n",
    "    Xn[:, 0]  = 1\n",
    "    Xn[:, 1:] = X\n",
    "    return Xn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Xn = add_intercept(X)\n",
    "Theta = gd(derivadas, loss, Xn, Y, lambda_=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previsoes(X, Theta):\n",
    "    P = softmax(X, Theta)\n",
    "    return P.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = previsoes(Xn, Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "        1, 1, 0, 0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       100\n",
      "           1       1.00      1.00      1.00       100\n",
      "\n",
      "    accuracy                           1.00       200\n",
      "   macro avg       1.00      1.00      1.00       200\n",
      "weighted avg       1.00      1.00      1.00       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y, y_p.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Com PyTorch NN Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A criação de uma rede com o PyTorch NN é trivial. Bastante similar com outras APIs de alto nível estilo Keras. Abaixo, defimos uma rede neural sequencial com uma única ativação densa de tamanho 10 saídas. Isto define uma função softmax de 10 classes.Você não precisa implementar nada. A ideia até mais em baixo é entender como a vida é mais simples ao usar a API do PyTorch NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputsDim, outDim = 28*28, 10 #Teremos imagens 28*28 e 10 classes de saida\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(inputsDim, outDim, bias = True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora indicamos que vamos fazer uso de uma entropia cruzada como função de perda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E que vamos treinar com gradiente descendente. SGD em particular!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos os passos que fizemos na mão se reduzem a poucas linhas\n",
    "\n",
    "#### Carregando Dados\n",
    "\n",
    "Agora vamos carregar os dados do Fashion Mnist. Leia [este link](https://github.com/SurhanZahid/Fashion-MNIST-Classifier-Pytorch) para uma descrição da base. O PyTorch Vision consegue baixar ela da internet.\n",
    "\n",
    "Antes de usar a base, vamos definir uma função de transformação para converter a mesma de imagens (28, 28) para vetores de (28 * 28) posições. Além do mais, converter as unidades para float32. Bibliotecas de aprendizado profundo como o pytorch são bem chatas com os tipos. É importante lembrar que os operadores de tensor recebam como input operandos de mesmo tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando a Base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformando as imagens em tensores 2D \n",
    "trainset = torchvision.datasets.FashionMNIST(root = \"./data\", train = True, download = True, transform = transforms.ToTensor())\n",
    "testset = torchvision.datasets.FashionMNIST(root = \"./data\", train = False, download = True, transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a classe `torch.utils.data.DataLoader` conseguimos iterar pela base em minibatches. Estamos usando aqui batches de tamanho 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=50, shuffle = True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinando\n",
    "\n",
    "Podemos treinar com um laço. Abaixo detalhamos o mesmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  0.5188412666320801\n",
      "epoch:  1  loss:  0.4720955789089203\n",
      "epoch:  2  loss:  0.7403064966201782\n",
      "epoch:  3  loss:  0.6215060949325562\n",
      "epoch:  4  loss:  0.5209754705429077\n",
      "epoch:  5  loss:  0.3291844129562378\n",
      "epoch:  6  loss:  0.2333328276872635\n",
      "epoch:  7  loss:  0.3119649291038513\n",
      "epoch:  8  loss:  0.488322913646698\n",
      "epoch:  9  loss:  0.6182224154472351\n",
      "epoch:  10  loss:  0.468759685754776\n",
      "epoch:  11  loss:  0.3305268883705139\n",
      "epoch:  12  loss:  0.6013189554214478\n",
      "epoch:  13  loss:  0.39560234546661377\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 15\n",
    "for epoch in range(max_epochs):\n",
    "    #iterate through all the batches in each epoch\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        #Coloca a rede em modo de treino   \n",
    "        model.train()     \n",
    "        inputs, labels = data  \n",
    "        inputs = inputs.view(50,1,28*28)\n",
    "        #Zera os gradientes (antes disso, eles contem o gradiente calculado no batch anterior)     \n",
    "        optimizer.zero_grad()  \n",
    "        \n",
    "        #Forward pass     \n",
    "        y_pred = model(inputs)\n",
    "        y_pred = y_pred.view(len(labels),-1) #Efetiva um flatten das imagens de entrada \n",
    "        loss = criterion(y_pred.float(), labels)    \n",
    "        \n",
    "        #Backward pass     \n",
    "        loss.backward()     \n",
    "        optimizer.step()     \n",
    "    print('epoch: ', epoch,' loss: ', loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abaixo, avaliamos a acuracia em teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y = []\n",
    "for i,data in enumerate(testloader,0):\n",
    "    inputs, labels = data\n",
    "    inputs = inputs.view(50,1,28*28)\n",
    "    y_pred.extend(model(inputs).argmax(2).tolist())\n",
    "    y.extend(labels.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([a[0] for a in y_pred])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_pred == y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perguntas\n",
    "\n",
    "1. Altere o treino para computar a acurácia no mesmo\n",
    "1. Brinque com a taxa de aprendizado do SGD e o número de iterações\n",
    "1. Qual o impacto na acurácia do treino/teste?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
