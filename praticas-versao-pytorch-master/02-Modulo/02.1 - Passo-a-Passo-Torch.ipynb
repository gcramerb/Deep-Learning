{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"02.1 - Passo-a-Passo-Torch.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ed03SC1Jm9Yy"},"source":["# Aprendizado Profundo - UFMG\n","\n","## Passo à Passo das Redes Neurais\n","\n","A criação e treinamento de uma rede neural tem alguns passos que foram um *pipeline* completo.\n","Nesta aula, vamos ver cada passo para criar e treinar uma rede neural do zero usando MXNet.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Gp6CwWnFnTwb"},"source":["Antes de começar, vamos instalar o MXNet. Esse pequeno bloco de código abaixo é usado somente para instalar o Pytorch para CUDA 10. Execute esse bloco somente uma vez e ignore possíveis erros levantados durante a instalação.\n","\n","Configure também o Collab para fazer uso de GPUs.\n","\n","Clique em \"Runtime\" depois \"Change Runtime\" e por altere as configurações da seguinte forma:\n","\n","![](https://www.kdnuggets.com/wp-content/uploads/colab-settings-1.png)"]},{"cell_type":"markdown","metadata":{"id":"ewFzsP61muMA","colab_type":"text"},"source":["O próximo bloco contém nossos imports. O mais importante aqui é o Torch. Torch é uma API mais alto nível para criação de redes neurais. Por baixo, temos um código estilo o de backpragation da aula passada. Porém, é bem melhor :-) Mais rápido, usa GPUs, carrega dados etc."]},{"cell_type":"markdown","metadata":{"id":"FPDhvI4hmuMB","colab_type":"text"},"source":["O próximo abaixo configura o torch para fazer uso de GPUs.\n","\n","**ATENÇÃO: a alteração deste bloco pode implicar em problemas na execução dos blocos restantes!**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XW-VATPAldgt","colab":{}},"source":["import torch\n","import torch.nn.functional as F\n","import torchvision\n","\n","from torchvision import datasets, transforms\n","from torch import optim, nn\n","\n","import os\n","import sys\n","import time\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WjkKUJ-zmuMG","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","plt.ion()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HZmFa64QmuMJ","colab_type":"code","outputId":"2ac4050d-3da9-41ac-a27e-da46ccdfba79","executionInfo":{"status":"ok","timestamp":1571255897564,"user_tz":180,"elapsed":1741,"user":{"displayName":"Victor Jorge","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAfOI6SYjf4fxfY0K7yBYAyYkR-mDmR7JiCZIq7Ew=s64","userId":"18258516524499027456"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Test if GPU is avaliable, if not, use cpu instead\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","n = torch.cuda.device_count()\n","devices_ids = list(range(n))\n","devices_ids"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Azv2ajIYkIjH"},"source":["## Passo 1: Carregar os dados\n","\n","Para treinar qualquer modelo de aprendizado de máquina, devemos carregar os dados.\n","Caso o modelo seja não supervisionado, teremos os dados somente.\n","Já para modelos treinados supervisionadamente, teremos os dados e os rótulos (também conhecidos como *labels* ou *ground-truths*).\n","\n","Observe como o TORCH importa dados, pelo menos os didáticos, de forma muito simples. Inicialmente o torch faz o carregamento das imagens no formato [Pillow](https://pillow.readthedocs.io/en/stable/). Portanto é necessário convertê-las para um tensor usando um transformer. O uso de transformers será detalhado adiante."]},{"cell_type":"code","metadata":{"id":"nzb-DN8sr7T_","colab_type":"code","colab":{}},"source":["mnist_train = datasets.MNIST('/', download=True, train=True, transform=transforms.ToTensor())\n","mnist_test = datasets.MNIST('/', download=True, train=False, transform=transforms.ToTensor())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jyO3F7lsmuMS","colab_type":"text"},"source":["Cada conjunto acima agora é um iterador. Podemos passar pelos elementos"]},{"cell_type":"code","metadata":{"id":"aM_HCRgbmuMT","colab_type":"code","outputId":"3c044e44-5ed5-465b-fb54-6c81a82f670b","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1571255928881,"user_tz":180,"elapsed":1150,"user":{"displayName":"Victor Jorge","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAfOI6SYjf4fxfY0K7yBYAyYkR-mDmR7JiCZIq7Ew=s64","userId":"18258516524499027456"}}},"source":["for xi, yi in mnist_train:\n","    print(xi.shape)\n","    print(yi)\n","    break"],"execution_count":5,"outputs":[{"output_type":"stream","text":["torch.Size([1, 28, 28])\n","5\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cBZOQVhEmuMX","colab_type":"text"},"source":["No loop acima, vemos que o treino é uma image de 28, 28 por 1. A resposta é uma classe. A base é bem similar com as outras que usamos junto com skimage."]},{"cell_type":"code","metadata":{"id":"-9FGvh5KmuMY","colab_type":"code","outputId":"66d451b8-caf8-455a-fdd6-6ca1c72a6311","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1571255930291,"user_tz":180,"elapsed":755,"user":{"displayName":"Victor Jorge","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAfOI6SYjf4fxfY0K7yBYAyYkR-mDmR7JiCZIq7Ew=s64","userId":"18258516524499027456"}}},"source":["for xi, yi in mnist_train:\n","    print(type(xi))\n","    print(type(yi))\n","    break"],"execution_count":6,"outputs":[{"output_type":"stream","text":["<class 'torch.Tensor'>\n","<class 'int'>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nvgX5W4omuMb","colab_type":"text"},"source":["O X é um TENSOR, o teste um inteiro. Como esperado, temos o número 5. Cada imagem é um número.\n","\n","Observe que precisei fazer um reshape para plotar, pois originalmente o torch trabalha com tensores. Então, converti para uma imagem: (28, 28, 1) -> (28, 28)"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"PsyGpjm3muMd","colab_type":"code","outputId":"798258cb-164f-44f0-bb78-ce0a9067b928","colab":{"base_uri":"https://localhost:8080/","height":275},"executionInfo":{"status":"ok","timestamp":1571255932772,"user_tz":180,"elapsed":1049,"user":{"displayName":"Victor Jorge","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAfOI6SYjf4fxfY0K7yBYAyYkR-mDmR7JiCZIq7Ew=s64","userId":"18258516524499027456"}}},"source":["for xi, yi in mnist_train:\n","    Img = xi.reshape((28, 28))\n","    plt.matshow(Img.numpy())\n","    break"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAQQAAAECCAYAAAAYUakXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQACQCAQAiUAAkP4X+3ZSmGRvad8AAAAASUVORK5CYII=\n","text/plain":["<Figure size 288x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"2P79XgfdmuMi","colab_type":"text"},"source":["Vamos supor que nossa ideia fosse fazer uso de um multiplayer perceptron simples. O mesmo trabalha em cima de uma vetor. Portanto, temos que converter imagem ao carregar. Para isto, podemos fazer uso de um Transformer. Existe um mundo de [Transformers](https://pytorch.org/docs/stable/torchvision/transforms.html). Use o comando help para entender os mesmos."]},{"cell_type":"code","metadata":{"id":"Ypn-5gC1muMj","colab_type":"code","outputId":"b11a7795-ad50-48f4-ade5-5548fcf969cc","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1571255971888,"user_tz":180,"elapsed":1172,"user":{"displayName":"Victor Jorge","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAfOI6SYjf4fxfY0K7yBYAyYkR-mDmR7JiCZIq7Ew=s64","userId":"18258516524499027456"}}},"source":["help(transforms.Compose)\n","help(transforms.ToTensor)\n","help(transforms.Resize)\n","help(transforms.Normalize)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Help on class Compose in module torchvision.transforms.transforms:\n","\n","class Compose(builtins.object)\n"," |  Composes several transforms together.\n"," |  \n"," |  Args:\n"," |      transforms (list of ``Transform`` objects): list of transforms to compose.\n"," |  \n"," |  Example:\n"," |      >>> transforms.Compose([\n"," |      >>>     transforms.CenterCrop(10),\n"," |      >>>     transforms.ToTensor(),\n"," |      >>> ])\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __call__(self, img)\n"," |      Call self as a function.\n"," |  \n"," |  __init__(self, transforms)\n"," |      Initialize self.  See help(type(self)) for accurate signature.\n"," |  \n"," |  __repr__(self)\n"," |      Return repr(self).\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors defined here:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables (if defined)\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object (if defined)\n","\n","Help on class ToTensor in module torchvision.transforms.transforms:\n","\n","class ToTensor(builtins.object)\n"," |  Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\n"," |  \n"," |  Converts a PIL Image or numpy.ndarray (H x W x C) in the range\n"," |  [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n"," |  if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1)\n"," |  or if the numpy.ndarray has dtype = np.uint8\n"," |  \n"," |  In the other cases, tensors are returned without scaling.\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __call__(self, pic)\n"," |      Args:\n"," |          pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n"," |      \n"," |      Returns:\n"," |          Tensor: Converted image.\n"," |  \n"," |  __repr__(self)\n"," |      Return repr(self).\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors defined here:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables (if defined)\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object (if defined)\n","\n","Help on class Resize in module torchvision.transforms.transforms:\n","\n","class Resize(builtins.object)\n"," |  Resize the input PIL Image to the given size.\n"," |  \n"," |  Args:\n"," |      size (sequence or int): Desired output size. If size is a sequence like\n"," |          (h, w), output size will be matched to this. If size is an int,\n"," |          smaller edge of the image will be matched to this number.\n"," |          i.e, if height > width, then image will be rescaled to\n"," |          (size * height / width, size)\n"," |      interpolation (int, optional): Desired interpolation. Default is\n"," |          ``PIL.Image.BILINEAR``\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __call__(self, img)\n"," |      Args:\n"," |          img (PIL Image): Image to be scaled.\n"," |      \n"," |      Returns:\n"," |          PIL Image: Rescaled image.\n"," |  \n"," |  __init__(self, size, interpolation=2)\n"," |      Initialize self.  See help(type(self)) for accurate signature.\n"," |  \n"," |  __repr__(self)\n"," |      Return repr(self).\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors defined here:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables (if defined)\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object (if defined)\n","\n","Help on class Normalize in module torchvision.transforms.transforms:\n","\n","class Normalize(builtins.object)\n"," |  Normalize a tensor image with mean and standard deviation.\n"," |  Given mean: ``(M1,...,Mn)`` and std: ``(S1,..,Sn)`` for ``n`` channels, this transform\n"," |  will normalize each channel of the input ``torch.*Tensor`` i.e.\n"," |  ``input[channel] = (input[channel] - mean[channel]) / std[channel]``\n"," |  \n"," |  .. note::\n"," |      This transform acts out of place, i.e., it does not mutates the input tensor.\n"," |  \n"," |  Args:\n"," |      mean (sequence): Sequence of means for each channel.\n"," |      std (sequence): Sequence of standard deviations for each channel.\n"," |      inplace(bool,optional): Bool to make this operation in-place.\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __call__(self, tensor)\n"," |      Args:\n"," |          tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n"," |      \n"," |      Returns:\n"," |          Tensor: Normalized Tensor image.\n"," |  \n"," |  __init__(self, mean, std, inplace=False)\n"," |      Initialize self.  See help(type(self)) for accurate signature.\n"," |  \n"," |  __repr__(self)\n"," |      Return repr(self).\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors defined here:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables (if defined)\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object (if defined)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"L6fSmUUhmuMm","colab_type":"text"},"source":["Vamos supor que você queira trabalhar com uma imagem em float64. Use um transformer!"]},{"cell_type":"code","metadata":{"id":"byiCopZPmuMn","colab_type":"code","outputId":"5d3c97eb-e655-45b1-c9b5-a74a01d439d9","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1571256091592,"user_tz":180,"elapsed":1032,"user":{"displayName":"Victor Jorge","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAfOI6SYjf4fxfY0K7yBYAyYkR-mDmR7JiCZIq7Ew=s64","userId":"18258516524499027456"}}},"source":["xi, yi = next(iter(mnist_train))\n","print(xi.dtype)\n","print(type(yi))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["torch.float32\n","<class 'int'>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BztxPlYdmuMv","colab_type":"text"},"source":["Podemos criar nossos transformers também! Por baixo são funções simples. Aqui usamos o tipo torch.cuda.FloatTensor porque estamos trabalhando em GPU. Podemos visualizar todos os tipos disponíveis [aqui](https://pytorch.org/docs/stable/tensors.html)."]},{"cell_type":"code","metadata":{"id":"OjfhHA6HmuMx","colab_type":"code","colab":{}},"source":["def transform(sample):\n","    return sample.reshape(28*28).type('torch.cuda.FloatTensor')\n","\n","custom_transform = transforms.Lambda(transform)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FRupb2RWmuM0","colab_type":"code","outputId":"78a1d930-69b4-40c3-bd4f-c451768ea714","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1571256097605,"user_tz":180,"elapsed":3374,"user":{"displayName":"Victor Jorge","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAfOI6SYjf4fxfY0K7yBYAyYkR-mDmR7JiCZIq7Ew=s64","userId":"18258516524499027456"}}},"source":["xi_new = transform(xi)\n","print(xi_new.shape)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["torch.Size([784])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kkgwvazNmuM4","colab_type":"text"},"source":["Jogamos o transformer no carregamento dos dados e voilà! Aqui é necessário usar uma composição de transformers. Basicamente, dizemos para o código de carregamento realizar as transformações na ordem em que a colocamos na lista que é passada como argumento da função Compose."]},{"cell_type":"code","metadata":{"id":"IVVkhL_YmuM5","colab_type":"code","colab":{}},"source":["transform_pipe = transforms.Compose([transforms.ToTensor(), custom_transform])\n","\n","mnist_train = datasets.MNIST('/', download=True, train=True, transform=transform_pipe)\n","mnist_test = datasets.MNIST('/', download=True, train=False, transform=transform_pipe)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ep0x6Q08muM9","colab_type":"code","outputId":"7efa1dc1-3bda-4efc-f8fb-70f0d8d0e290","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1571256110855,"user_tz":180,"elapsed":617,"user":{"displayName":"Victor Jorge","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAfOI6SYjf4fxfY0K7yBYAyYkR-mDmR7JiCZIq7Ew=s64","userId":"18258516524499027456"}}},"source":["for xi, yi in mnist_train:\n","    print(xi.shape)\n","    print(xi.dtype)\n","    break"],"execution_count":13,"outputs":[{"output_type":"stream","text":["torch.Size([784])\n","torch.float32\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Jeapok_-muNB","colab_type":"text"},"source":["Com a classe `gluon.data.DataLoader` conseguimos iterar pela base em minibatches. Esta é a ideia para executar um Minibatch-GD (abaixo)."]},{"cell_type":"code","metadata":{"id":"fmhdO1c5muNC","colab_type":"code","colab":{}},"source":["mnist_train = datasets.MNIST('/', download=True, train=True, transform=transform_pipe)\n","mnist_test = datasets.MNIST('/', download=True, train=False, transform=transform_pipe)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QKWZzaqXmuNG","colab_type":"code","colab":{}},"source":["minibatch_train = torch.utils.data.DataLoader(mnist_train, batch_size=50)\n","minibatch_test = torch.utils.data.DataLoader(mnist_test, batch_size=50)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jbHDxG1tmuNM","colab_type":"code","outputId":"c7a778fe-42df-4449-95f1-6894649501db","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1571256119138,"user_tz":180,"elapsed":1169,"user":{"displayName":"Victor Jorge","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAfOI6SYjf4fxfY0K7yBYAyYkR-mDmR7JiCZIq7Ew=s64","userId":"18258516524499027456"}}},"source":["for xi, yi in minibatch_train:\n","    print(xi.shape) # note como carregamos 50 vetores de tamanho 784\n","    print(xi.dtype)\n","    break"],"execution_count":16,"outputs":[{"output_type":"stream","text":["torch.Size([50, 784])\n","torch.float32\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"itYXKR3ZmuNP","colab_type":"text"},"source":["Uma prática comum que você pode ter nos seus dados é criar pastas de imagem de treino, teste e validação. Não temos isto pronto no notebook mas seria algo como o código abaixo. Note o uso de um `torchvision.datasets.ImageFolder`.\n","\n","```python\n","# loading the data and apply pre-processing(transforms) on images\n","train_data = torch.utils.data.DataLoader(\n","    datasets.ImageFolder(train_path), transform=transformer,\n","    batch_size=batch_size, shuffle=True, num_workers=num_workers)\n","\n","val_data = torch.utils.data.DataLoader(\n","    datasets.ImageFolder(val_path), transform=transformer,\n","    batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","\n","test_data = torch.utils.data.DataLoader(\n","    datasets.ImageFolder(test_path), transform=transformer,\n","    batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","```"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rRTTtpx4FhOC"},"source":["## Passo 2: Definir a arquitetura\n","\n","Um modelo deve ser definido para ser treinado usando os dados desejados.\n","No caso de redes neurais, criaremos uma nova arquitetura.\n","Tal arquitetura pode ser composta de diversos tipos de camadas como, por exemplo, [Densas ou Lineares](https://pytorch.org/docs/stable/nn.html#linear) (camadas totalmente conectadas como as que implementamos do zero), [convolucionais](https://pytorch.org/docs/stable/nn.html#convolution-layers), e [recorrentes](https://pytorch.org/docs/stable/nn.html#recurrent-layers).\n","Veremos todas essas camadas ao longo do curso.\n","\n","Primeiramente definimos uma função que faz a inicialização dos parâmetros da rede."]},{"cell_type":"code","metadata":{"id":"-yZonh4wKy4c","colab_type":"code","colab":{}},"source":["def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Linear') != -1:\n","        m.weight.data.normal_(0.0, 0.01) # valores iniciais são uma normal"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NqnvSWigK1a9","colab_type":"text"},"source":["O código abaixo cria uma rede simples MLP. Note como a mesma tem 10 saídas. A entrada é definida, que é o formato que transformamos nossa imagem de entrada."]},{"cell_type":"code","metadata":{"id":"km5sXrE1muNR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"b5458553-d1a9-458f-a68f-f2f904a6050e","executionInfo":{"status":"ok","timestamp":1571256347026,"user_tz":180,"elapsed":1128,"user":{"displayName":"Victor Jorge","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAfOI6SYjf4fxfY0K7yBYAyYkR-mDmR7JiCZIq7Ew=s64","userId":"18258516524499027456"}}},"source":["net = nn.Sequential(\n","    nn.Linear(784, 10)\n",")\n","\n","net.apply(weights_init) # aplica a função de inicialização às camadas da rede"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sequential(\n","  (0): Linear(in_features=784, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BAgtOMZKHqS9"},"source":["## Passo 3: Definir a função de custo (função de perda ou *loss*)\n","\n","Funções de perda, também conhecidas como *loss functions*, são muito importantes para o aprendizagem de máquinas, pois servem como uma forma de medir a distância ou a diferença entre a saída prevista de um modelo e o seu valor real, auxiliando então no treino no modelo.\n","\n","Diversas funções de perda foram propostas ao longo do tempo para diferentes tipos de problemas.\n","Algumas dessas funções foram propostas para auxiliar no treino de modelos de regressão linear, como a *loss* [L1](https://pytorch.org/docs/stable/nn.html#l1loss).\n","Já outras foram propostas para problemas de classificação, como [Hinge](https://pytorch.org/docs/stable/nn.html#hingeembeddingloss), e [Cross Entropy](https://pytorch.org/docs/stable/nn.html#crossentropyloss).\n","\n","Veremos várias funções de custo mais adiante no curso.\n","Neste primeiro momento, focaremos na função [Cross Entropy](https://pytorch.org/docs/stable/nn.html#crossentropyloss) combinada com a ativação [Softmax](https://pytorch.org/docs/stable/nn.html#softmax), pois é a combinação mais comum nos dias de hoje para a tarefa de classificação.\n","\n","O pequeno bloco de código abaixo implementa essa função de perda em torch."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WVoEMNgBIt9v","colab":{}},"source":["# função de custo (ou loss)\n","cross_entropy = nn.CrossEntropyLoss()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BDJGM6GrJIeV"},"source":["## Passo 4: Definir o algoritmo de otimização\n","\n","Definimos uma arquitetura, composta de diversos pesos geralmente inicializados aleatoriamente, e definimos também uma função que custo que nos permite avaliar o quão bem esta rede neural está.\n","Entretanto, para que possamos alterar os pesos e fazer a rede convergir para um bom resultados, devemos definir um algoritmo de otimização, que usará derivadas parciais da função de custo em relação àos pesos para otimizar a rede.\n","\n","Diversos algoritmos foram propostos ao longo dos anos como, por exemplo, Stochastic Gradient Descent (SGD), Adam, e RMSProp.\n","Veremos mais adianta sobre cada um desses algoritmos.\n","Nesse primeiro momento, usaremos o algoritmo mais comum: Stochastic Gradient Descent (SGD). A documentação de como usar esses diferentes otimizadores com o torch pode ser encontrada [aqui](https://pytorch.org/docs/stable/optim.html#algorithms).\n","\n","O código abaixo implementa uma função de otimização usando o SGD para otimizar todos os parâmetros da rede neural proposta."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SvJ1W-niJIGA","colab":{}},"source":["# trainer do torch\n","lr = 0.5\n","trainer = optim.SGD(net.parameters(), lr=lr)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-F4glSYTmuNd","colab_type":"text"},"source":["### Treinando\n","\n","A partir deste momento já podemos treinar! Observe como a biblioteca permite de forma muito simples uma implementação do gradiente descendente"]},{"cell_type":"code","metadata":{"id":"MTrp2UOymuNe","colab_type":"code","colab":{}},"source":["# 1. Carrega dados\n","mnist_train = datasets.MNIST('/', download=True, train=True, transform=transform_pipe)\n","mnist_test = datasets.MNIST('/', download=True, train=False, transform=transform_pipe)\n","\n","# 2. Define mini-batch\n","minibatch_train = torch.utils.data.DataLoader(mnist_train, batch_size=50)\n","minibatch_test = torch.utils.data.DataLoader(mnist_test, batch_size=50)\n","\n","# 3. Define rede\n","\n","net = nn.Sequential(\n","    nn.Linear(784, 10)\n",")\n","\n","net.apply(weights_init)\n","net.to(device) # diz para a rede que ela deve ser treinada na GPU\n","\n","# 4. Define loss e treinamento\n","cross_entropy = nn.CrossEntropyLoss()\n","lr = 0.01\n","trainer = optim.SGD(net.parameters(), lr=lr)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LuGJVWa1muNh","colab_type":"text"},"source":["Agora, treine!"]},{"cell_type":"code","metadata":{"id":"qDgFxUI_muNh","colab_type":"code","outputId":"91613d0e-5ecd-4de7-a594-e14544911292","colab":{"base_uri":"https://localhost:8080/","height":107},"executionInfo":{"status":"ok","timestamp":1571256445899,"user_tz":180,"elapsed":53389,"user":{"displayName":"Victor Jorge","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAfOI6SYjf4fxfY0K7yBYAyYkR-mDmR7JiCZIq7Ew=s64","userId":"18258516524499027456"}}},"source":["iteracoes_treino = 5\n","for i in range(iteracoes_treino):         # número de iterações, não verificamos convergencia\n","    cumulative_loss = 0\n","    for data, y in minibatch_train:       # para cada minibatch\n","        data, y = data.to(device), y.to(device)\n","\n","        trainer.zero_grad()\n","\n","        P = net(data)                     # execute o softmax, retorne as probabilidades, forward\n","        loss = cross_entropy(P, y)        # compute a perda\n","        loss.backward()                   # atualiza os pesos com backward, nosso backprop de antes\n","        trainer.step()                    # atualize os parâmetros com a derivada\n","        cumulative_loss += loss.item()\n","    print('Iteração {}. Perda {}'.format(i, cumulative_loss / len(data)))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Iteração 0. Perda 21.470576854646207\n","Iteração 1. Perda 12.254552741646767\n","Iteração 2. Perda 10.632046619057656\n","Iteração 3. Perda 9.832181369662285\n","Iteração 4. Perda 9.331509895175696\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NsJt6AjxmuNk","colab_type":"text"},"source":["Podemos treinar aquiteturas mais complicadas. Abaixo, treinaremos uma arquitetura baseada na [LeNet-5](https://ieeexplore.ieee.org/document/726791).\n","\n","Notem que, além de definir o tipo da camada, é necessário também definir a sua ativação.\n","Existem diversos tipos de ativações como, por exemplo, [ReLU](https://pytorch.org/docs/stable/nn.html#relu), [LeakyReLU](https://pytorch.org/docs/stable/nn.html#leakyrelu), e [sigmoid](https://pytorch.org/docs/stable/nn.html#sigmoid).\n","Neste primeiro momento, usaremos somente a ReLU.\n","Porém, veremos mais de perto várias outras ativações ao longo deste curso.\n","\n","O trecho de código abaixo implementa uma rede simples usando [ReLUs](https://pytorch.org/docs/stable/nn.html#relu) no framework torch."]},{"cell_type":"code","metadata":{"id":"j7g0fuDpmuNl","colab_type":"code","colab":{}},"source":["# 1. Carrega dados\n","mnist_train = datasets.MNIST('/', download=True, train=True, transform=transform_pipe)\n","mnist_test = datasets.MNIST('/', download=True, train=False, transform=transform_pipe)\n","\n","# 2. Define mini-batch\n","minibatch_train = torch.utils.data.DataLoader(mnist_train, batch_size=50)\n","minibatch_test = torch.utils.data.DataLoader(mnist_test, batch_size=50)\n","\n","# 3. Define rede\n","\n","net = nn.Sequential(\n","    nn.Linear(784, 128),\n","    nn.ReLU(),\n","    nn.Linear(128, 64),\n","    nn.ReLU(),\n","    nn.Linear(64, 10)\n",")\n","\n","net.apply(weights_init)\n","net.to(device) # diz para a rede que ela deve ser treinada na GPU\n","\n","# 4. Define loss e treinamento\n","cross_entropy = nn.CrossEntropyLoss()\n","lr = 0.01\n","trainer = optim.SGD(net.parameters(), lr=lr)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RyGG3HvgmuNo","colab_type":"text"},"source":["Observe como nessa rede nova a perda é bem menor logo de cara, parece melhor!"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FAEBKuhzHLeO","outputId":"104ef982-6ef6-4378-f9f3-597637798dc0","colab":{"base_uri":"https://localhost:8080/","height":107},"executionInfo":{"status":"ok","timestamp":1571256508973,"user_tz":180,"elapsed":56758,"user":{"displayName":"Victor Jorge","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAfOI6SYjf4fxfY0K7yBYAyYkR-mDmR7JiCZIq7Ew=s64","userId":"18258516524499027456"}}},"source":["iteracoes_treino = 5\n","for i in range(iteracoes_treino):         # número de iterações, não verificamos convergencia\n","    cumulative_loss = 0\n","    for data, y in minibatch_train:       # para cada minibatch\n","        data, y = data.to(device), y.to(device) \n","\n","        trainer.zero_grad()\n","\n","        P = net(data)                     # execute o softmax, retorne as probabilidades, forward\n","        loss = cross_entropy(P, y)        # compute a perda\n","        loss.backward()                   # atualiza os pesos com backward, nosso backprop de antes\n","        trainer.step()                    # atualize os parâmetros com a derivada\n","        cumulative_loss += loss.item()\n","    print('Iteração {}. Perda {}'.format(i, cumulative_loss / len(data)))"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Iteração 0. Perda 55.23879845142365\n","Iteração 1. Perda 55.138643593788146\n","Iteração 2. Perda 50.74311968564987\n","Iteração 3. Perda 26.898302388191222\n","Iteração 4. Perda 15.673906292915344\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9lgetWqDmuNt","colab_type":"text"},"source":["Abaixo rodamos um minibatch apenas. Note como a rede gera previsões. Para cada instância, temos 10 elementos da camada final. A probabilidade de cada classe!"]},{"cell_type":"code","metadata":{"id":"2fjIXan9muNu","colab_type":"code","outputId":"7f02a879-a5bb-4296-acf5-0649235d666e","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1571256586692,"user_tz":180,"elapsed":1006,"user":{"displayName":"Victor Jorge","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAfOI6SYjf4fxfY0K7yBYAyYkR-mDmR7JiCZIq7Ew=s64","userId":"18258516524499027456"}}},"source":["for data, y in minibatch_test: \n","    P = net(data)                # faça uma previsão\n","    print(P.shape)\n","    break"],"execution_count":26,"outputs":[{"output_type":"stream","text":["torch.Size([50, 10])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"N0DtuUoamuNx","colab_type":"text"},"source":["Abaixo avaliamos a acurácia no teste."]},{"cell_type":"code","metadata":{"id":"oreiP3_kmuNy","colab_type":"code","outputId":"abbc4e9f-883b-4f44-96b1-bbdf85210800","colab":{}},"source":["i = 0\n","with torch.no_grad():\n","    for data, y in minibatch_test:            # para cada minibatch do teste\n","        P = net(data.to(device))              # faça uma previsão\n","        _, predicted = torch.max(P, 1)        # pegue a classe mais provavel\n","        c = (predicted == labels).squeeze()\n","    \n","print(acc)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["EvalMetric: {'accuracy': 0.1028}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YkxA9w4sLX1L"},"source":["## Passo 5: Colando Tudo Junto\n","\n","\n","No geral, não vamos fazer isso sempre do zero. \n","\n","Portanto, abaixo implementamos algumas funções auxiliares. As mesmas carregam os dados, quebram treinam e validam, reportam métricas. Vamos usar elas nos outros notebooks.\n","\n","Portanto, criamos conjuntos de treino/validação. Este é último passo para então treinar a rede neural. Abixo, temos uma função que que recebe os dados, a rede, o *loss*, e o algoritmo de otimização, e realmente converge o modelo."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tlBGUroCFfyh","colab":{}},"source":["## carregando dados\n","\n","# código para carregar o dataset do MNIST\n","# http://yann.lecun.com/exdb/mnist/\n","def load_data_mnist(batch_size, resize=None, root=os.path.join(\n","        '~', '.pytorch', 'datasets', 'fashion-mnist')):\n","    \"\"\"Download the Fashion-MNIST dataset and then load into memory.\"\"\"\n","    root = os.path.expanduser(root)\n","    transformer = []\n","    if resize:\n","        transformer += [transforms.Resize(resize)]\n","    transformer += [transforms.ToTensor()]\n","    transformer = transforms.Compose(transformer)\n","\n","    mnist_train = datasets.MNIST(root=root, train=True,download=True, transform=transformer)\n","    mnist_test = datasets.MNIST(root=root, train=False,download=True, transform=transformer)\n","    num_workers = 0 if sys.platform.startswith('win32') else 4\n","\n","\n","\n","    train_iter = torch.utils.data.DataLoader(mnist_train,\n","                                  batch_size, shuffle=True,\n","                                  num_workers=num_workers)\n","    test_iter = torch.utils.data.DataLoader(mnist_test,\n","                                 batch_size, shuffle=False,\n","                                 num_workers=num_workers)\n","    return train_iter, test_iter\n","\n","# funções básicas\n","def _get_batch(batch):\n","    \"\"\"Return features and labels on ctx.\"\"\"\n","    features, labels = batch\n","    if labels.type() != features.type():\n","        labels = labels.type(features.type())\n","    return (torch.nn.DataParallel(features, device_ids=devices_ids),\n","            torch.nn.DataParallel(labels, device_ids=devices_ids), features.shape[0])\n","\n","# Função usada para calcular acurácia\n","def evaluate_accuracy(data_iter, net, loss):\n","    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n","\n","    acc_sum, n, l = torch.Tensor([0]), 0, 0\n","    \n","    with torch.no_grad():\n","      for X, y in data_iter:\n","          #y = y.astype('float32')\n","          X, y = X.to(device), y.to(device)\n","          y_hat = net(X)\n","          l += loss(y_hat, y).sum()\n","          acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n","          n += y.size()[0]\n","\n","    return acc_sum.item() / n, l.item() / len(data_iter)\n","  \n","# Função usada no treinamento e validação da rede\n","def train_validate(net, train_iter, test_iter, batch_size, trainer, loss,\n","                   num_epochs):\n","    print('training on', device)\n","    for epoch in range(num_epochs):\n","        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n","        for X, y in train_iter:\n","            X, y = X.to(device), y.to(device)\n","            y_hat = net(X)\n","            trainer.zero_grad()\n","            l = loss(y_hat, y).sum()\n","            l.backward()\n","            trainer.step()\n","            train_l_sum += l.item()\n","            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n","            n += y.size()[0]\n","        test_acc, test_loss = evaluate_accuracy(test_iter, net, loss)\n","        print('epoch %d, train loss %.4f, train acc %.3f, test loss %.4f, '\n","              'test acc %.3f, time %.1f sec'\n","              % (epoch + 1, train_l_sum / len(train_iter), train_acc_sum / n, test_loss, \n","                 test_acc, time.time() - start))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oi-7Y6LVmuN4","colab_type":"code","colab":{}},"source":["# carregamento do dado: mnist\n","batch_size = 256\n","train_iter, test_iter = load_data_mnist(batch_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RkghPp7smuN7","colab_type":"code","colab":{}},"source":["net = nn.Sequential(\n","    nn.Flatten(), # Camada para transformar a imagem 2D em um vetor 1D\n","    nn.Linear(784, 128),\n","    nn.ReLU(),\n","    nn.Linear(128, 64),\n","    nn.ReLU(),\n","    nn.Linear(64, 10)\n",")\n","\n","net.apply(weights_init)\n","net.to(device)\n","\n","cross_entropy = nn.CrossEntropyLoss()\n","lr = 0.01\n","trainer = optim.SGD(net.parameters(), lr=lr)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mJyHotTImuN_","colab_type":"code","outputId":"329e13c3-8e3a-44a4-cca7-ebac3f88c310","colab":{"base_uri":"https://localhost:8080/","height":215},"executionInfo":{"status":"ok","timestamp":1571259640282,"user_tz":180,"elapsed":68888,"user":{"displayName":"Victor Jorge","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAfOI6SYjf4fxfY0K7yBYAyYkR-mDmR7JiCZIq7Ew=s64","userId":"18258516524499027456"}}},"source":["num_epochs = 10\n","train_validate(net, train_iter, test_iter, batch_size, trainer, cross_entropy, num_epochs)"],"execution_count":39,"outputs":[{"output_type":"stream","text":["training on cuda\n","epoch 1, train loss 2.3029, train acc 0.112, test loss 2.3028, test acc 0.114, time 6.7 sec\n","epoch 2, train loss 2.3020, train acc 0.112, test loss 2.3019, test acc 0.114, time 6.8 sec\n","epoch 3, train loss 2.3014, train acc 0.112, test loss 2.3013, test acc 0.114, time 6.8 sec\n","epoch 4, train loss 2.3009, train acc 0.112, test loss 2.3008, test acc 0.114, time 6.8 sec\n","epoch 5, train loss 2.3005, train acc 0.112, test loss 2.3002, test acc 0.114, time 6.8 sec\n","epoch 6, train loss 2.2999, train acc 0.112, test loss 2.2996, test acc 0.114, time 6.8 sec\n","epoch 7, train loss 2.2992, train acc 0.112, test loss 2.2987, test acc 0.114, time 6.8 sec\n","epoch 8, train loss 2.2982, train acc 0.112, test loss 2.2973, test acc 0.114, time 6.8 sec\n","epoch 9, train loss 2.2964, train acc 0.112, test loss 2.2949, test acc 0.114, time 6.8 sec\n","epoch 10, train loss 2.2931, train acc 0.114, test loss 2.2901, test acc 0.128, time 6.8 sec\n"],"name":"stdout"}]}]}