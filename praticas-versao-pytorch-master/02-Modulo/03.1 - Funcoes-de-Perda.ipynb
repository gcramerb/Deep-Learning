{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"03.1 - Funcoes-de-Perda.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ed03SC1Jm9Yy"},"source":["# Aprendizado Profundo - UFMG\n","\n","## Funções de perda (*loss functions*)\n","\n","Neste código iremos analisar diferentes funções de perda (também conhecidas como *loss functions*) que são usadas para avaliar a rede no estado atual.\n","\n","Funções de perda, também conhecidas como *loss functions*, são muito importantes para o aprendizagem de máquinas, pois servem como uma forma de medir a distância ou a diferença entre a saída prevista de um modelo e o seu valor real, auxiliando então no treino no modelo.\n","\n","Diversas funções de perda foram propostas ao longo do tempo para diferentes tipos de problemas.\n","Algumas dessas funções foram propostas para auxiliar no treino de modelos de regressão linear, como as *loss* [L1](https://pytorch.org/docs/stable/nn.html#l1loss), [L2](https://pytorch.org/docs/stable/nn.html#mseloss) e [Huber](https://pytorch.org/docs/stable/nn.html#smoothl1loss).\n","Outras foram propostas para serem usadas em problemas de classificação, como a mais comum de todas [Cross-Entropy](https://pytorch.org/docs/stable/nn.html#crossentropyloss).\n","\n","<p align=\"center\">\n","  <img src=\"https://drive.google.com/uc?export=view&id=1ITV4Ikw0NP39p1KNFkt46LWwnhzLKf7h\">\n","</p>\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Gp6CwWnFnTwb"},"source":["Esse pequeno bloco de código abaixo é usado somente para instalar o MXNet para CUDA 10. Execute esse bloco somente uma vez e ignore possíveis erros levantados durante a instalação.\n","\n","**ATENÇÃO: a alteração dos blocos pode implicar em problemas na execução dos blocos restantes!**"]},{"cell_type":"markdown","metadata":{"id":"EFmg1Dxv5Bzg","colab_type":"text"},"source":["## Preâmbulo"]},{"cell_type":"code","metadata":{"id":"6QBk4NCu5Bzl","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn.functional as F\n","import torchvision\n","\n","from torchvision import datasets, transforms\n","from torch import optim, nn\n","\n","from sklearn.model_selection import train_test_split\n","\n","import numpy as np\n","\n","import os\n","import sys\n","import time"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mEUbwZXN5Bzn","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","plt.ion()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rZyvGCCq5Bzq","colab_type":"code","outputId":"b0ec0c17-39d1-42fd-d5b5-b6fdb62559f6","executionInfo":{"status":"ok","timestamp":1572009473009,"user_tz":180,"elapsed":1929,"user":{"displayName":"Victor Jorge","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAfOI6SYjf4fxfY0K7yBYAyYkR-mDmR7JiCZIq7Ew=s64","userId":"18258516524499027456"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Test if GPU is avaliable, if not, use cpu instead\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","n = torch.cuda.device_count()\n","devices_ids = list(range(n))\n","devices_ids"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"g9u0pCOtlWLu","colab":{}},"source":["## carregando dados básicos\n","\n","# dados sintéticos somente para \n","def synthetic_regression_data(w, b, num_examples):\n","    \"\"\"generate y = X w + b + noise\"\"\"\n","    X = np.random.normal(scale=1, size=(num_examples, len(w)))\n","    y = np.dot(X, w) + b\n","    y += np.random.normal(scale=0.01, size=y.shape)\n","    return torch.FloatTensor(X), torch.FloatTensor(y)\n","\n","# código para carregar o dataset do MNIST\n","# http://yann.lecun.com/exdb/mnist/\n","def load_data_mnist(batch_size, resize=None, root=os.path.join(\n","        '~', '.pytorch', 'datasets', 'fashion-mnist')):\n","    \"\"\"Download the Fashion-MNIST dataset and then load into memory.\"\"\"\n","    root = os.path.expanduser(root)\n","    transformer = []\n","    if resize:\n","        transformer += [transforms.Resize(resize)]\n","    transformer += [transforms.ToTensor()]\n","    transformer = transforms.Compose(transformer)\n","\n","    mnist_train = datasets.MNIST(root=root, train=True,download=True, transform=transformer)\n","    mnist_test = datasets.MNIST(root=root, train=False,download=True, transform=transformer)\n","    num_workers = 0 if sys.platform.startswith('win32') else 4\n","\n","\n","\n","    train_iter = torch.utils.data.DataLoader(mnist_train,\n","                                  batch_size, shuffle=True,\n","                                  num_workers=num_workers)\n","    test_iter = torch.utils.data.DataLoader(mnist_test,\n","                                 batch_size, shuffle=False,\n","                                 num_workers=num_workers)\n","    return train_iter, test_iter"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8oSVf8u1Oi1m","colab":{}},"source":["# funções básicas\n","\n","def load_array(features, labels, batch_size, is_train=True):\n","    \"\"\"Construct a Gluon data loader\"\"\"\n","    dataset = torch.utils.data.TensorDataset(features, labels)\n","    return torch.utils.data.DataLoader(dataset, batch_size, shuffle=is_train)\n","\n","def _get_batch(batch):\n","    \"\"\"Return features and labels on ctx.\"\"\"\n","    features, labels = batch\n","    if labels.type() != features.type():\n","        labels = labels.type(features.type())\n","    return (torch.nn.DataParallel(features, device_ids=devices_ids),\n","            torch.nn.DataParallel(labels, device_ids=devices_ids), features.shape[0])\n","\n","# Função usada para calcular acurácia\n","def evaluate_accuracy(data_iter, net, loss):\n","    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n","\n","    acc_sum, n, l = torch.Tensor([0]), 0, 0\n","    \n","    with torch.no_grad():\n","      for X, y in data_iter:\n","          #y = y.astype('float32')\n","          X, y = X.to(device), y.to(device)\n","          y_hat = net(X)\n","          l += loss(y_hat, y).sum()\n","          acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n","          n += y.size()[0]\n","\n","    return acc_sum.item() / n, l.item() / len(data_iter)\n","  \n","# Função usada no treinamento e validação da rede\n","def train_validate(net, train_iter, test_iter, batch_size, trainer, loss,\n","                   num_epochs, type='regression'):\n","    print('training on', device)\n","    for epoch in range(num_epochs):\n","        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n","        for X, y in train_iter:\n","            X, y = X.to(device), y.to(device)\n","            y_hat = net(X)\n","            l = loss(y_hat, y)\n","            l.backward()\n","            trainer.step()\n","            trainer.zero_grad()\n","            train_l_sum += l.item()\n","            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n","            n += y.size()[0]\n","        test_acc, test_loss = evaluate_accuracy(test_iter, net, loss)\n","        if type == 'regression':\n","          print('epoch %d, train loss %.4f, test loss %.4f, time %.1f sec'\n","                % (epoch + 1, train_l_sum / len(train_iter), test_loss, time.time() - start))\n","        else:\n","          print('epoch %d, train loss %.4f, train acc %.3f, test loss %.4f, '\n","              'test acc %.3f, time %.1f sec'\n","              % (epoch + 1, train_l_sum / len(train_iter), train_acc_sum / n, test_loss, \n","                 test_acc, time.time() - start))\n","          \n","# Função para inicializar pesos da rede\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Linear') != -1:\n","        m.weight.data.normal_(0.0, 0.01) # valores iniciais são uma normal"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Azv2ajIYkIjH"},"source":["## *Loss* L2\n","\n","O função de custo chamada L2 (também conhecida como *Mean Squared Error* -- MSE) é, talvez, a função de perda mais simples e comum. \n","Essa função é representada simplesmente pela média do quadrado da diferença entre as previsões do modelo e o *ground-truth*.\n","Essa função nunca terá valores negativos, pois a diferença calculada será sempre elevado à segunda potência.\n","\n","Formalmente, dado a valor real $y$, e a predição feita pelo modelo $\\hat{y}$, a *loss* L2 é definida pela seguinte equação:\n","\n","$$\\mathcal{l}_2^i(w, b) = \\frac{1}{2} (\\hat{y}^i - y^i)^2 $$\n","\n","A constante $1/2$ é apenas por conveniência matemática, garantindo que depois de tomarmos a derivada dessa função, o coeficiente constante será de $1$.\n","\n","A grande vantagem dessa função é que ela garante que o modelo treinado não tenha previsões discrepantes com erros enormes, já que ela atribui maior peso a esses erros devido à parte quadrática da função.\n","Entretanto, isso gera a desvantagem dessa função de custo, pois se o modelo faz uma única previsão muito ruim, a parte quadrática da função aumenta o erro consideravelmente.\n","No entanto, em muitos casos práticos, não nos importamos muito com esses poucos valores discrepantes e buscamos um modelo mais abrangente que tenha um bom desempenho na maioria.\n","\n","Para tentar garantir a qualidade do modelo em todo o conjunto de dados, podemos simplesmente calcular a média das perdas no conjunto de treinamento:\n","\n","$$\\mathcal{L}(w, b) = \\frac{1}{n} \\sum_i^N \\mathcal{l}^{i}_2(w, b) $$\n","\n","### Implementação\n","\n","Em frameworks atuais (como no MxNet, TensorFlow, e PyTorch), a implementação de funções de custo comuns, como a L2, são diretas e muitos simples.\n","\n","**Um exemplo é mostrado abaixo utilizando o framework PyTorch.**"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1572010855328,"user_tz":180,"elapsed":733,"user":{"displayName":"Victor Jorge","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAfOI6SYjf4fxfY0K7yBYAyYkR-mDmR7JiCZIq7Ew=s64","userId":"18258516524499027456"}},"id":"T0LIS1C8S6k1","outputId":"0ffaa584-b99a-4472-bd04-95c712d91617","colab":{"base_uri":"https://localhost:8080/","height":143}},"source":["seed = [2, -3.4]\n","seed_gt = 4.2\n","features, labels = synthetic_regression_data(seed, seed_gt, 1000)\n","  \n","batch_size = 10\n","data_iter = load_array(features, labels, batch_size)\n","\n","# arquitetura super simples\n","net = nn.Sequential(\n","    nn.Linear(2,1)\n",")\n","\n","net.apply(weights_init)\n","net.to(device) # diz para a rede que ela deve ser treinada na GPU\n","\n","loss = nn.MSELoss()  # loss L2\n","trainer = optim.SGD(net.parameters(), lr=0.03)\n","\n","# treino\n","num_epochs = 3\n","for epoch in range(1, num_epochs + 1):\n","    loss_sum = 0.0\n","    for X, y in data_iter:\n","        X, y = X.to(device), y.to(device)\n","        y_hat = net(X).view(batch_size)\n","        l = loss(y_hat, y)\n","        l.backward()\n","        trainer.step()\n","        trainer.zero_grad()\n","        loss_sum += l.item()\n","    print('epoch %d, loss: %f' % (epoch, l.mean().item()))\n","\n","for i in range(995, 999):\n","  y_hat = net(features[i:i+1, :].to(device))\n","  print(y_hat, labels[i], labels[i] - y_hat)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["epoch 1, loss: 0.000567\n","epoch 2, loss: 0.000165\n","epoch 3, loss: 0.000043\n","tensor([[3.0942]], device='cuda:0', grad_fn=<AddmmBackward>) tensor(3.1060) tensor([[0.0118]], device='cuda:0', grad_fn=<SubBackward0>)\n","tensor([[7.8048]], device='cuda:0', grad_fn=<AddmmBackward>) tensor(7.8078) tensor([[0.0030]], device='cuda:0', grad_fn=<SubBackward0>)\n","tensor([[1.9249]], device='cuda:0', grad_fn=<AddmmBackward>) tensor(1.9184) tensor([[-0.0065]], device='cuda:0', grad_fn=<SubBackward0>)\n","tensor([[4.8392]], device='cuda:0', grad_fn=<AddmmBackward>) tensor(4.8476) tensor([[0.0083]], device='cuda:0', grad_fn=<SubBackward0>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xaR--CY3GnP7","outputId":"0c52ad0f-a0fe-443f-f81a-de88bdf3a4d5","executionInfo":{"status":"ok","timestamp":1572010208851,"user_tz":180,"elapsed":979,"user":{"displayName":"Victor Jorge","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAfOI6SYjf4fxfY0K7yBYAyYkR-mDmR7JiCZIq7Ew=s64","userId":"18258516524499027456"}},"colab":{"base_uri":"https://localhost:8080/","height":91}},"source":["loss(net(features[:10, :].to(device)), labels[:10].to(device))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.l1_loss(input, target, reduction=self.reduction)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["tensor(2.7124, device='cuda:0', grad_fn=<L1LossBackward>)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lz_ayDVLnNFU"},"source":["## *Loss* L1\n","\n","A função de custo L1 é apenas ligeiramente diferente da L2, mas fornece curiosamente propriedades quase exatamente opostas!\n","Essa função é representada pelo valor absoluto da diferença entre as previsões do modelo e o *ground-truth*.\n","\n","Essa função, assim como o *loss* L2, nunca será negativo, pois neste caso estamos sempre assumindo o valor absoluto dos erros.\n","Formalmente, dado a valor real (*ground-truth*) $y$, e a predição feita pelo modelo $\\hat{y}$, a *loss* L1 é definida pela seguinte equação:\n","\n","$$\\mathcal{l}_1^i(w, b) = \\sum_i |\\hat{y}^i - y^i| $$\n","\n","A grande vantagem da função de custo L1 cobre diretamente a desvantagem do *loss* L2.\n","Em outras palaras, como estamos trabalhando com o valor absoluto, todos os erros serão ponderados na mesma escala linear.\n","Assim, ao contrário do *loss* L2, não estamos colocando muito peso nos valores com grande discrepância e a função de perda fornece uma medida genérica e uniforme do desempenho do modelo.\n","\n","Por outro lado, a desvantagem desta função é, para alguns casos, não dar pesos diferentes para específico valores discrepantes. Por exemplo, os erros relativamente grandes provenientes dos *outliers* acabam sendo ponderados exatamente como erros menores. Isso pode resultar em nosso modelo sendo ótimo na maior parte do tempo, mas fazendo algumas previsões muito ruins de vez em quando.\n","\n","Essa função pode ser facilmente implementada no PyTorch, como pode ser visto [aqui](https://pytorch.org/docs/stable/nn.html#l1loss)."]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1572010894020,"user_tz":180,"elapsed":739,"user":{"displayName":"Victor Jorge","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAfOI6SYjf4fxfY0K7yBYAyYkR-mDmR7JiCZIq7Ew=s64","userId":"18258516524499027456"}},"id":"gYe6ZR3L9jxc","outputId":"d5e87c9e-c1a2-4dbc-df2a-5301577c9448","colab":{"base_uri":"https://localhost:8080/","height":143}},"source":["seed = [2, -3.4]\n","seed_gt = 4.2\n","features, labels = synthetic_regression_data(seed, seed_gt, 1000)\n","  \n","batch_size = 10\n","data_iter = load_array(features, labels, batch_size)\n","\n","# arquitetura super simples\n","net = nn.Sequential(\n","    nn.Linear(2,1)\n",")\n","\n","net.apply(weights_init)\n","net.to(device) # diz para a rede que ela deve ser treinada na GPU\n","\n","loss = nn.L1Loss()  # loss L1\n","trainer = optim.SGD(net.parameters(), lr=0.03)\n","\n","# treino\n","num_epochs = 3\n","for epoch in range(1, num_epochs + 1):\n","    loss_sum = 0.0\n","    for X, y in data_iter:\n","        X, y = X.to(device), y.to(device)\n","        y_hat = net(X).view(batch_size)\n","        l = loss(y_hat, y)\n","        l.backward()\n","        trainer.step()\n","        trainer.zero_grad()\n","        loss_sum += l.item()\n","    print('epoch %d, loss: %f' % (epoch, l.mean().item()))\n","\n","for i in range(995, 999):\n","  y_hat = net(features[i:i+1, :].to(device))\n","  print(y_hat, labels[i], labels[i] - y_hat)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["epoch 1, loss: 2.363916\n","epoch 2, loss: 0.300343\n","epoch 3, loss: 0.016175\n","tensor([[6.4369]], device='cuda:0', grad_fn=<AddmmBackward>) tensor(6.4389) tensor([[0.0020]], device='cuda:0', grad_fn=<SubBackward0>)\n","tensor([[10.8079]], device='cuda:0', grad_fn=<AddmmBackward>) tensor(10.8299) tensor([[0.0220]], device='cuda:0', grad_fn=<SubBackward0>)\n","tensor([[2.8072]], device='cuda:0', grad_fn=<AddmmBackward>) tensor(2.8070) tensor([[-0.0002]], device='cuda:0', grad_fn=<SubBackward0>)\n","tensor([[2.0541]], device='cuda:0', grad_fn=<AddmmBackward>) tensor(2.0411) tensor([[-0.0130]], device='cuda:0', grad_fn=<SubBackward0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FfLtl5pE-TYJ"},"source":["## Huber *Loss* \n","\n","Vimos que a função de perda L2 tem certas vantangens (como conseguir aprender *outliers*), enquanto o *loss* L1 tem outros benefícios, como ignorar os *outliers*.\n","Porém, existe uma forma de combinar e agregar os benefícios das duas?\n","\n","Sim! A Huber *Loss* oferece o melhor dos dois mundos, equilibrando as funções de perda L1 e L2 juntos. \n","Formalmente, dado a valor real (*ground-truth*) $y$, e a predição feita pelo modelo $\\hat{y}$, a Huber *Loss* é definida pela seguinte equação:\n","\n","$$\n","l_H^i(w, b) = \\sum_i \\begin{cases}\n","                                        \\frac{1}{2\\rho} (\\hat{y}^i - y^i)^2, & \\text{if } |\\hat{y}^i - y^i| < \\rho\\\\\n","                                        |\\hat{y}^i - y^i| - \\frac{\\rho}{2},  & \\text{otherwise}\n","\\end{cases}\n","$$\n",", onde $\\rho$ é uma constance que define a margem.\n","O que essa equação essencialmente diz é: para valores de perda menores que $\\rho$, use o *loss& L2; para valores de perda maiores que delta, use a função de custo L1.\n","Isso efetivamente combina o melhor dos dois mundos das duas funções de perda!\n","\n","O uso da função de custo L1 para valores maiores reduz o peso que colocamos em valores discrepantes para que possamos obter um modelo completo. Ao mesmo tempo, usamos o *loss* L2 para valores menores de perda para manter uma função quadrática próxima ao centro.\n","\n","Essa função pode ser facilmente implementada no PyTorch, como pode ser visto [aqui](https://pytorch.org/docs/stable/nn.html#smoothl1loss)."]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1572011046602,"user_tz":180,"elapsed":794,"user":{"displayName":"Victor Jorge","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAfOI6SYjf4fxfY0K7yBYAyYkR-mDmR7JiCZIq7Ew=s64","userId":"18258516524499027456"}},"id":"98gc8lje-MCL","outputId":"e4c8df05-156a-45b9-9a19-481e57dfbef8","colab":{"base_uri":"https://localhost:8080/","height":143}},"source":["seed = [2, -3.4]\n","seed_gt = 4.2\n","features, labels = synthetic_regression_data(seed, seed_gt, 1000)\n","  \n","batch_size = 10\n","data_iter = load_array(features, labels, batch_size)\n","\n","# arquitetura super simples\n","net = nn.Sequential(\n","    nn.Linear(2,1)\n",")\n","\n","net.apply(weights_init)\n","net.to(device) # diz para a rede que ela deve ser treinada na GPU\n","\n","loss = nn.SmoothL1Loss()  # loss L1\n","trainer = optim.SGD(net.parameters(), lr=0.03)\n","\n","# treino\n","num_epochs = 3\n","for epoch in range(1, num_epochs + 1):\n","    loss_sum = 0.0\n","    for X, y in data_iter:\n","        X, y = X.to(device), y.to(device)\n","        y_hat = net(X).view(batch_size)\n","        l = loss(y_hat, y)\n","        l.backward()\n","        trainer.step()\n","        trainer.zero_grad()\n","        loss_sum += l.item()\n","    print('epoch %d, loss: %f' % (epoch, l.mean().item()))\n","\n","for i in range(995, 999):\n","  y_hat = net(features[i:i+1, :].to(device))\n","  print(y_hat, labels[i], labels[i] - y_hat)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["epoch 1, loss: 1.713221\n","epoch 2, loss: 0.411430\n","epoch 3, loss: 0.001395\n","tensor([[6.2705]], device='cuda:0', grad_fn=<AddmmBackward>) tensor(6.3045) tensor([[0.0340]], device='cuda:0', grad_fn=<SubBackward0>)\n","tensor([[10.9950]], device='cuda:0', grad_fn=<AddmmBackward>) tensor(11.0803) tensor([[0.0853]], device='cuda:0', grad_fn=<SubBackward0>)\n","tensor([[3.2220]], device='cuda:0', grad_fn=<AddmmBackward>) tensor(3.2706) tensor([[0.0486]], device='cuda:0', grad_fn=<SubBackward0>)\n","tensor([[6.3679]], device='cuda:0', grad_fn=<AddmmBackward>) tensor(6.4135) tensor([[0.0456]], device='cuda:0', grad_fn=<SubBackward0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8fG_HV8k1YOe"},"source":["## *Loss Cross-Entropy*\n","\n","O função de custo chamada *cross-entropy* ou *log loss* é a mais usada em problemas de classificação.\n","Essa função de perda, embasada pela teoria da informação, procura penalizar o *loss* baseado somente na classe correta de cada amostra.\n","\n","Formalmente, dado a valor real $y$, e a predição feita pelo modelo $\\hat{y}$, a *loss cross-entropy* é definida pela seguinte equação:\n","\n","$$\\mathcal{l}(w, b) = - \\sum_i y_i log~\\hat{y}_i $$\n",", onde $\\hat{y}$ é saída normalizada (via [softmax](https://pytorch.org/docs/stable/nn.html#softmax)) da predição da rede.\n","\n","Em particular, no somatório apenas um termo será diferente de zero e esse termo será o $log$ da probabilidade (normalizada via [softmax](https://pytorch.org/docs/stable/nn.html#softmax)) atribuída à classe correta. Intuitivamente, isso faz sentido porque $log (x)$ está aumentando no intervalo (0,1), então $−log (x)$ está diminuindo naquele intervalo.\n","Por exemplo, se tivermos uma amostra com probabilidade de 0.8 para o rótulo correto, o *loos* será penalizado em apenas 0.09.\n","Já se tivermos uma probabilidade menor de 0.08, o *loss* será penalizado em 1,09.\n","\n","### Implementação\n","\n","Em frameworks atuais (como no MxNet, TensorFlow, e PyTorch), a implementação da função de custo *cross-entropy* é direta.\n","\n","**Um exemplo é mostrado abaixo utilizando o framework PyTorch.**"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1572011592880,"user_tz":180,"elapsed":157131,"user":{"displayName":"Victor Jorge","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAfOI6SYjf4fxfY0K7yBYAyYkR-mDmR7JiCZIq7Ew=s64","userId":"18258516524499027456"}},"id":"EFzoRhT51mW_","outputId":"03b8d416-203f-41a5-d695-7f68d87f8fbb","colab":{"base_uri":"https://localhost:8080/","height":395}},"source":["# parâmetros: número de epochs, learning rate (ou taxa de aprendizado), e \n","# tamanho do batch\n","num_epochs, lr, batch_size = 20, 0.5, 256\n","\n","# rede simples somente com perceptrons e camadas densamente conectadas\n","net = nn.Sequential(\n","        nn.Flatten(),\n","        nn.Linear(784, 256),\n","        nn.ReLU(),\n","        nn.Linear(256, 128),\n","        nn.ReLU(),\n","        nn.Linear(128, 64),\n","        nn.ReLU(),\n","        nn.Linear(64, 10))\n","\n","net.apply(weights_init)\n","net.to(device) # diz para a rede que ela deve ser treinada na GPU\n","\n","# função de custo (ou loss)\n","loss = nn.CrossEntropyLoss()\n","\n","# carregamento do dado: mnist\n","train_iter, test_iter = load_data_mnist(batch_size)\n","\n","# trainer do torch\n","trainer = optim.SGD(net.parameters(), lr=lr)\n","\n","# treinamento e validação via MXNet\n","train_validate(net, train_iter, test_iter, batch_size, trainer, loss, num_epochs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["training on cuda\n","epoch 1, train loss 2.3017, test loss 2.3006, time 7.8 sec\n","epoch 2, train loss 1.8956, test loss 1.2487, time 7.8 sec\n","epoch 3, train loss 0.6408, test loss 0.2172, time 7.8 sec\n","epoch 4, train loss 0.4282, test loss 0.5747, time 7.8 sec\n","epoch 5, train loss 0.2140, test loss 0.1427, time 7.8 sec\n","epoch 6, train loss 0.1136, test loss 0.1274, time 7.8 sec\n","epoch 7, train loss 0.0896, test loss 0.1214, time 7.9 sec\n","epoch 8, train loss 0.0724, test loss 0.0979, time 7.9 sec\n","epoch 9, train loss 0.0761, test loss 0.1001, time 7.9 sec\n","epoch 10, train loss 0.0529, test loss 0.0847, time 7.8 sec\n","epoch 11, train loss 0.0443, test loss 0.3610, time 7.8 sec\n","epoch 12, train loss 0.3256, test loss 0.1209, time 7.8 sec\n","epoch 13, train loss 0.1075, test loss 0.1263, time 7.8 sec\n","epoch 14, train loss 0.0699, test loss 0.0994, time 7.8 sec\n","epoch 15, train loss 0.0560, test loss 0.0994, time 7.8 sec\n","epoch 16, train loss 0.0452, test loss 0.0947, time 7.8 sec\n","epoch 17, train loss 0.0390, test loss 0.1152, time 7.7 sec\n","epoch 18, train loss 0.0344, test loss 0.1446, time 7.8 sec\n","epoch 19, train loss 0.0302, test loss 0.0961, time 7.7 sec\n","epoch 20, train loss 0.0253, test loss 0.1030, time 7.8 sec\n"],"name":"stdout"}]}]}